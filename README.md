# Kaggle-LLM Prompt Recovery

## 1. Project Overview

In the field of Natural Language Processing (NLP), Large Language Models (LLMs) are widely used for text rewriting and style transformation tasks. However, the "black-box" nature of LLMs makes it challenging to trace the specific prompts that guide text rewriting. The Kaggle-LLM Prompt Recovery project aims to address this key problem: given pairs of original text and LLM-rewritten text, recover the hidden prompt that instructed the LLM to perform the rewriting. This project not only contributes to understanding the mechanism of LLM-guided text transformation but also provides technical support for optimizing prompt engineering and improving LLM controllability.

## 2. Dependencies Installation

```bash
pip install -r requirements.txt
```

`requirements.txt` includes the following core dependencies:

```bash
torch==2.1.0

transformers==4.38.2

sentence-transformers==2.2.2

faiss-gpu==1.7.2

bitsandbytes==0.41.1

peft==0.4.0

pandas==2.1.4

numpy==1.26.3

scikit-learn==1.3.2

tqdm==4.66.1
```

## 3. Dataset Introduction

### 3.1 Dataset Details

*   **Official Dataset**: Provided by Kaggle, containing 1,300+ pairs of original text and rewritten text generated by the Gemma-7B-IT model, with limited training samples.

*   **Supplementary Dataset**: Constructed to address data scarcity, including:


    *   Original text: Sourced from the OpenWebText dataset (filtered to remove texts longer than 512 tokens).
    
    *   Rewrite prompts: Generated in batches using OpenAI's GPT-3.5-Turbo API, covering diverse style transformation tasks (e.g., "rewrite as a warning," "convert to poetic style").
    
    *   Rewritten text: Generated by the Gemma-7B-IT model using original text and rewrite prompts as input.

### 3.2 Data Processing

*   Cleaned low-quality samples (e.g., texts with missing prompts, overly short rewritten text).

*   Filtered invalid content (e.g., texts containing irrelevant links, garbled characters).

*   Saved processed data in Parquet format for efficient loading: `train_clean.parquet` (training set) and `validation826.csv` (validation set).

## 4. Methodology

### 4.1 Framework Design: Multi-Model Ensemble

The project adopts a three-model ensemble strategy to improve prompt recovery accuracy, combining strengths of different model architectures.

#### 4.1.1 Seq2Seq Model

*   **Backbone**: Deberta-v3-large, a robust pre-trained language model.

*   **Feature Extraction**:


    *   Uses Mean Pooling to extract semantic features from original text and rewritten text.
    
    *   Concatenates features of original and rewritten text to capture transformation relationships.

*   **Head Network**: A multi-layer perceptron (MLP) mapping concatenated features to prompt embeddings (768-dimensional).

*   **Loss Function**: Cosine Embedding Loss, optimizing the similarity between predicted prompt embeddings and ground-truth embeddings.

#### 4.1.2 Few-Shot LLM (Mistral-7B-Instruct-v0.2)

*   **Prompt Engineering**: Designs in-context learning prompts with 6+ examples, each containing "original text → rewritten text → prompt" triples.

*   **Task Description**: Guides the model to infer prompts by analyzing differences between original and rewritten text (e.g., "identify style changes and generate corresponding prompts").

*   **Output Constraints**: Trims results to 1-2 sentences to ensure conciseness and relevance.

#### 4.1.3 Phi2 SFT Model

*   **Fine-Tuning**: Performs supervised fine-tuning (SFT) on the Phi2 model using high-quality prompt-text pairs.

*   **Input Format**: Constructs prompts as `Instruct: Original Text:{ori_text}\nRewritten Text:{rew_text}\nWrite a prompt...Output:`.

*   **Generation Strategy**: Uses greedy decoding to generate prompts, ensuring consistency with training data patterns.

#### 4.1.4 Ensemble Strategy

*   Concatenates outputs of the three models: `Seq2Seq_prompt + Phi2_prompt + Mistral_prompt`.

*   Uses FAISS to build a prompt embedding index for efficient similarity search and candidate selection.

## 5. Experimental Setup

### 5.1 Baseline Models

*   Single Seq2Seq model (Deberta-v3-large).

*   Zero-shot LLM (without in-context examples).

*   Non-ensemble Phi2 model.

### 5.2 Evaluation Metric

*   **Sharpened Cosine Similarity (SCS)**:

$ 
  \text{Similarity} = \cos(\theta) = \frac{A \cdot B}{\|A\| \|B\|}, \quad \text{Score} = \text{Similarity}^3

$

Computed using embeddings from the `sentence-transformers/sentence-t5-base` model, with the cubic term reducing overestimation of low-quality prompts.

### 5.3 Training Configuration

*   **Optimizer**: AdamW with betas=(0.9, 0.999) and weight decay=0.008.

*   **Learning Rate**: 5e-6 (Seq2Seq model), 3e-5 (Phi2 SFT).

*   **Epochs**: 6 (Seq2Seq), 5 (Phi2 SFT).

*   **Hardware**: NVIDIA GPU (≥16GB VRAM) for model training and inference.

*   **Regularization**: Gradient clipping (max norm=100000.0) and early stopping.

## 6. Experimental Results

The multi-model ensemble outperforms all baseline models on the validation set, with key results as follows:

| Model              | Validation SCS Score |
| ------------------ | -------------------- |
| Single Seq2Seq     | 0.52                 |
| Zero-shot Mistral  | 0.48                 |
| Single Phi2 SFT    | 0.55                 |
| **Ensemble Model** | **0.63**             |

**Key Findings**:

*   The Seq2Seq model excels at capturing semantic correlations between text pairs.

*   Few-shot Mistral-7B enhances prompt diversity via in-context learning.

*   Phi2 SFT improves prompt style consistency through fine-tuning.

*   Ensemble strategy integrates complementary strengths, achieving the highest score.

## 7. Innovations

1.  **Multi-Model Fusion**: Combines discriminative (Seq2Seq) and generative (LLM/Phi2) models to balance accuracy and diversity.

2.  **Retrieval-Enhanced Generation**: Uses FAISS to index prompts, enabling efficient similarity-based candidate refinement.

3.  **Prompt Engineering Optimization**: Designs task-specific in-context examples for few-shot learning, improving LLM inference alignment.

## 8. References

1.  Kaggle Competition: [LLM Prompt Recovery](https://www.kaggle.com/competitions/llm-prompt-recovery).

2.  Devlin J, et al. "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding." NAACL-HLT (2019).

3.  Touvron H, et al. "Mistral 7B: A New SOTA for Open-Source LLMs." arXiv preprint arXiv:2310.06825 (2023).

4.  Gao L, et al. "Phi-2: Small Language Models with Great Performance." Microsoft Research Blog (2023).

5.  Reimers N, Gurevych I. "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks." EMNLP (2019).
